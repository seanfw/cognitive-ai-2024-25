{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Training RNNs on cognitive tasks"
      ],
      "metadata": {
        "id": "bH8uH8rx6MTg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Uninstall the current Gym version\n",
        "# !pip uninstall -y gym\n",
        "\n",
        "# # Install Gym version 0.23.1\n",
        "# !pip install gym==0.23.1\n",
        "\n",
        "# # Restart the runtime after installation (necessary in some environments like Colab)\n",
        "# import os\n",
        "# os._exit(00)\n"
      ],
      "metadata": {
        "id": "YTRThwxByWbI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "838e2e25-acf8-4da9-f768-1994b1e2f3e1"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Introduction\n",
        "\n",
        "In lab 1 we explored the architecture and dynamics of Recurrent Neural Networks (RNNs). Now, we transition from understanding the mechanics of RNNs to deploying them effectively on cognitive tasks. We’ll explore how these networks, inspired by the recurrent connections in our brain, can be trained to perform tasks that mimic cognitive functions. Engaging in such exercises not only offers insights into artificial intelligence but also sheds light on the computational capabilities of our own neural circuits.\n",
        "\n",
        "We will train our network to perform a perceptual decision-making task. In the laboratory, the test subject (human or animal) is shown moving dots on a screen, and must respond to indicate whether most dots are moving to the left or right. By recording from different brain areas, neuroscientists have been able to isolate the brain areas where the evidence accumulates in order to make this type of perceptual decision [(review paper)](https://www.cell.com/neuron/fulltext/S0896-6273(13)00999-9?script=true&code=cell-site).\n",
        "Let's take a closer look at how this cognitive task is performed in real life to deepen our understanding. Here is a [link](https://www.youtube.com/watch?v=oDxcyTn-0os&ab_channel=PamelaReinagelatUCSD) to a video featuring a rat executing this perceptual decision-making task.\n",
        "\n",
        "We will build and train our network using pytorch, and then do the same using only numpy, to understand how the pytorch magic works."
      ],
      "metadata": {
        "id": "qtd39htCstwB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's proceed with our main topic for today - training RNNs on cognitive tasks!"
      ],
      "metadata": {
        "id": "jzU7LCTOKwof"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HNsCB508r3UZ"
      },
      "source": [
        "### Installing and importing relevant packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "drHbsMbKr3Ua",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d61cd746-4148-419f-e1a3-e92e8144c142"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'neurogym' already exists and is not an empty directory.\n",
            "/content/neurogym/neurogym\n",
            "Obtaining file:///content/neurogym/neurogym\n",
            "\u001b[31mERROR: file:///content/neurogym/neurogym does not appear to be a Python project: neither 'setup.py' nor 'pyproject.toml' found.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "# Install neurogym to use cognitive tasks\n",
        "! git clone https://github.com/neurogym/neurogym.git\n",
        "%cd neurogym/\n",
        "! pip install -e ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "BYIY5KJtr3Ua"
      },
      "outputs": [],
      "source": [
        "# Import common packages\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VjpUvzGNr3Ua"
      },
      "source": [
        "## Defining a recurrent neural network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "incorporated-editor"
      },
      "source": [
        "In general, recurrent neural networks transform **sequence to sequence**. In the context of cognitive neuroscience, the sequence is usually a time series of task input or output. Recall the sequence we produced in Tutorial 1 by executing a forward pass through an RNN?\n",
        "\n",
        "Let's understand the input and output dimensions of a typical recurrent network in machine learning, LSTM networks.\n",
        "\n",
        "(Usage example adopted from pytorch documentation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "outputId": "269bc075-f791-4e2c-9b5c-8cbdc2b6f1f3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0VGmZ7mtr3Ub"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output shape is (SeqLen, BatchSize, HiddenSize): torch.Size([5, 3, 20])\n"
          ]
        }
      ],
      "source": [
        "# Make a LSTM, input_size is the dimension of inputs,\n",
        "# hidden_size is the number of hidden neurons\n",
        "rnn = nn.LSTM(input_size=10, hidden_size=20, num_layers=2)\n",
        "\n",
        "# Generate some mock inputs\n",
        "input = torch.randn(5, 3, 10)  # The arguments represent (Sequence Length, Batch Size, Input Size). Typically, in neuroscience,\n",
        "# sequence length would correspond to time points in the time series, Batch size corresponds to the number of trials and\n",
        "# input size corresponds to the dimension of the input (ie., the number of neurons or channels you're collecting data from)\n",
        "output, (hn, cn) = rnn(input)\n",
        "\n",
        "print('Output shape is (SeqLen, BatchSize, HiddenSize):', output.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Defining a Leaky Recurrent Neural Network (Leaky RNN)**"
      ],
      "metadata": {
        "id": "xg9fY5C4_snE"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "proper-armor"
      },
      "source": [
        "Neuroscientists often prefer **Leaky Recurrent Neural Networks (Leaky RNNs)** due to their ability to accurately model the continuous and dynamic nature of biological neural processes. Leaky RNNs can mimic the temporal dynamics and adaptive learning capabilities of biological neural networks, providing a closer approximation to real neurological processes. Furthermore, their robustness in handling noisy environments, capability to generate complex behaviors, and applicability in studying real-time interactions and sensorimotor coordination make them a valuable tool in neuroscience research and experimentation.\n",
        "\n",
        "Let us define a continuous-time leaky recurrent neural network,\n",
        "\\begin{align}\n",
        "    \\tau \\frac{d\\mathbf{a}}{dt} = -\\mathbf{a}(t) + f(W_{a\\rightarrow\n",
        "a} \\mathbf{a}(t) + W_{x\\rightarrow a} \\mathbf{x}(t) + \\mathbf{b}_1).\n",
        "\\end{align}\n",
        "\n",
        "Where,\n",
        "\n",
        "$a(t)$ is the vector of neural firing rates (or activations) at time $t$.\n",
        "\n",
        "$τ$ is the time constant which determines how fast the state approaches its steady-state value.\n",
        "\n",
        "$f$ is a non-linear activation function applied element-wise.\n",
        "\n",
        "$W_{a\\rightarrow a}$ is the recurrent weight matrix.\n",
        "\n",
        "$x(t)$ is the input vector at time $t$.\n",
        "\n",
        "$W_{x\\rightarrow a}$ is the input weight matrix.\n",
        "\n",
        "$b_1​$ is the bias vector.\n",
        "\n",
        "\n",
        "Let us discretize this network in time using the Euler method with a time step of $\\Delta t$,\n",
        "\\begin{align}\n",
        "    \\mathbf{a}(t+\\Delta t) = \\mathbf{a}(t) + \\Delta \\mathbf{a} &= \\mathbf{a}(t) + \\frac{\\Delta t}{\\tau}[-\\mathbf{a}(t) + f(W_{a\\rightarrow a} \\mathbf{a}(t) + W_{x\\rightarrow a}  \\mathbf{x}(t) + \\mathbf{b}_r)] \\\\\n",
        "    &= (1 - \\frac{\\Delta t}{\\tau})\\mathbf{a}(t) + \\frac{\\Delta t}{\\tau}f(W_{a\\rightarrow a} \\mathbf{a}(t) + W_{x\\rightarrow a}  \\mathbf{x}(t) + \\mathbf{b}_r)\n",
        "\\end{align}"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us now define the network following the dynamics described by the above equation."
      ],
      "metadata": {
        "id": "friRVLXaMegj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "coupled-sessions"
      },
      "outputs": [],
      "source": [
        "class LeakyRNN(nn.Module):\n",
        "    \"\"\"Leaky RNN.\n",
        "\n",
        "    Parameters:\n",
        "        input_size: Number of input neurons\n",
        "        hidden_size: Number of hidden neurons\n",
        "        dt: discretization time step in ms.\n",
        "            If None, dt equals time constant tau\n",
        "\n",
        "    Inputs:\n",
        "        input: tensor of shape (seq_len, batch, input_size)\n",
        "        hidden: tensor of shape (batch, hidden_size), initial hidden activity\n",
        "            if None, hidden is initialized through self.init_hidden()\n",
        "\n",
        "    Outputs:\n",
        "        output: tensor of shape (seq_len, batch, hidden_size)\n",
        "        hidden: tensor of shape (batch, hidden_size), final hidden activity\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_size, hidden_size, dt=None, **kwargs):\n",
        "        super().__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.tau = 100\n",
        "        if dt is None:\n",
        "            alpha = 1\n",
        "        else:\n",
        "            alpha = dt / self.tau\n",
        "        self.alpha = alpha\n",
        "\n",
        "        self.input2h = nn.Linear(input_size, hidden_size)\n",
        "        self.h2h = nn.Linear(hidden_size, hidden_size)\n",
        "\n",
        "    def init_hidden(self, input_shape):\n",
        "        batch_size = input_shape[1]\n",
        "        return torch.zeros(batch_size, self.hidden_size)\n",
        "\n",
        "    def recurrence(self, input, hidden):\n",
        "        \"\"\"Run network for one time step.\n",
        "\n",
        "        Inputs:\n",
        "            input: tensor of shape (batch, input_size)\n",
        "            hidden: tensor of shape (batch, hidden_size)\n",
        "\n",
        "        Outputs:\n",
        "            h_new: tensor of shape (batch, hidden_size),\n",
        "                network activity at the next time step\n",
        "        \"\"\"\n",
        "        h_new = torch.relu(self.input2h(input) + self.h2h(hidden))\n",
        "        h_new = hidden * (1 - self.alpha) + h_new * self.alpha\n",
        "        return h_new\n",
        "\n",
        "    def forward(self, input, hidden=None):\n",
        "        \"\"\"Propogate input through the network.\"\"\"\n",
        "\n",
        "        # If hidden activity is not provided, initialize it\n",
        "        if hidden is None:\n",
        "            hidden = self.init_hidden(input.shape).to(input.device)\n",
        "\n",
        "        # Loop through time\n",
        "        output = []\n",
        "        steps = range(input.size(0))\n",
        "        for i in steps:\n",
        "            hidden = self.recurrence(input[i], hidden)\n",
        "            output.append(hidden)\n",
        "\n",
        "        # Stack together output from all time steps\n",
        "        output = torch.stack(output, dim=0)  # (seq_len, batch, hidden_size)\n",
        "        return output, hidden\n",
        "\n",
        "\n",
        "class RNNNet(nn.Module):\n",
        "    \"\"\"Recurrent network model.\n",
        "\n",
        "    Parameters:\n",
        "        input_size: int, input size\n",
        "        hidden_size: int, hidden size\n",
        "        output_size: int, output size\n",
        "\n",
        "    Inputs:\n",
        "        x: tensor of shape (Seq Len, Batch, Input size)\n",
        "\n",
        "    Outputs:\n",
        "        out: tensor of shape (Seq Len, Batch, Output size)\n",
        "        rnn_output: tensor of shape (Seq Len, Batch, Hidden size)\n",
        "    \"\"\"\n",
        "    def __init__(self, input_size, hidden_size, output_size, **kwargs):\n",
        "        super().__init__()\n",
        "\n",
        "        # Leaky RNN\n",
        "        self.rnn = LeakyRNN(input_size, hidden_size, **kwargs)\n",
        "\n",
        "        # Add an output layer\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        rnn_output, _ = self.rnn(x)\n",
        "        out = self.fc(rnn_output)\n",
        "        return out, rnn_output"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's determine the dimensions of its inputs and outputs."
      ],
      "metadata": {
        "id": "zscQyqJKAPsn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 16\n",
        "seq_len = 20  # sequence length\n",
        "input_size = 5  # input dimension\n",
        "\n",
        "# Make some random inputs\n",
        "input_rnn = torch.rand(seq_len, batch_size, input_size)\n",
        "\n",
        "# Make network of 100 hidden units and 10 output units\n",
        "rnn = RNNNet(input_size=input_size, hidden_size=100, output_size=10)\n",
        "\n",
        "# Run the sequence through the network\n",
        "out, rnn_output = rnn(input_rnn)\n",
        "\n",
        "print('Input of shape =', input_rnn.shape)\n",
        "print('Output of shape =', out.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cJiMYOS0A4QP",
        "outputId": "894f9e2b-a3e5-469d-de52-19e38dafaa4e"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input of shape = torch.Size([20, 16, 5])\n",
            "Output of shape = torch.Size([20, 16, 10])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "artificial-michael"
      },
      "source": [
        "## Defining a simple cognitive task"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we use the neurogym package to make a simple \"perceptual decision making\" task. Let us install the package first.NeuroGym is a curated collection of neuroscience tasks with a common interface. You may explore further [here](https://github.com/neurogym/neurogym)\n",
        "\n",
        "The code provided below defines a custom environment, PerceptualDecisionMaking, using neurogym. This environment simulates a two-alternative forced choice task where an agent needs to decide which of two stimuli is higher on average, despite the stimuli being noisy. The agent is encouraged to integrate the stimulus over time due to this noise.\n",
        "\n",
        "Given that the focus of today's tutorial is on training an RNN, after you browse the neurogym website, you can skip over much of this section (except for the first cell), or study at your own pace."
      ],
      "metadata": {
        "id": "Aq0JWZh1JCFW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "u3KQN9OCB_c8"
      },
      "outputs": [],
      "source": [
        "# @title importing neurogym\n",
        "import neurogym as ngym\n",
        "\n",
        "# Canned environment from neurogym\n",
        "\n",
        "task_name = 'PerceptualDecisionMaking-v0'\n",
        "\n",
        "# Importantly, we set discretization time step for the task as well\n",
        "kwargs = {'dt': 20, 'timing': {'stimulus': 1000}}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6yro2ByHB_c9"
      },
      "source": [
        "For **supervised learning**, we need a dataset that returns (input, target output pairs)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "sa-AtjsVB_c9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6715a469-f92b-4996-c67a-3acc45c63d9c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input has shape (SeqLen, Batch, Dim) = torch.Size([100, 16, 3])\n",
            "Target has shape (SeqLen, Batch) = (100, 16)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:69: UserWarning: \u001b[33mWARN: Agent's minimum action space value is -infinity. This is probably too low.\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:73: UserWarning: \u001b[33mWARN: Agent's maximum action space value is infinity. This is probably too high\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:97: UserWarning: \u001b[33mWARN: We recommend you to use a symmetric and normalized Box action space (range=[-1, 1]) https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html\u001b[0m\n",
            "  logger.warn(\n"
          ]
        }
      ],
      "source": [
        "# Make supervised dataset\n",
        "seq_len = 100\n",
        "batch_size = 16\n",
        "dataset = ngym.Dataset(task_name, env_kwargs=kwargs, batch_size=batch_size, seq_len=seq_len)\n",
        "env = dataset.env\n",
        "\n",
        "# Generate one batch of data when called\n",
        "inputs, target = dataset()\n",
        "inputs = torch.from_numpy(inputs).type(torch.float)\n",
        "\n",
        "input_size = env.observation_space.shape[0]\n",
        "output_size = env.action_space.n\n",
        "\n",
        "print('Input has shape (SeqLen, Batch, Dim) =', inputs.shape)\n",
        "print('Target has shape (SeqLen, Batch) =', target.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "separate-message"
      },
      "source": [
        "## Network Training\n",
        "\n",
        "Let's now train the network to perform the task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "romantic-recognition",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "464e0757-c659-44db-91a9-dfd7bcb5cb67"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RNNNet(\n",
            "  (rnn): LeakyRNN(\n",
            "    (input2h): Linear(in_features=3, out_features=128, bias=True)\n",
            "    (h2h): Linear(in_features=128, out_features=128, bias=True)\n",
            "  )\n",
            "  (fc): Linear(in_features=128, out_features=3, bias=True)\n",
            ")\n",
            "Training network...\n",
            "Step 100, Loss 0.1945, Time 7.5s\n",
            "Step 200, Loss 0.0782, Time 17.2s\n",
            "Step 300, Loss 0.0607, Time 22.5s\n",
            "Step 400, Loss 0.0447, Time 26.4s\n",
            "Step 500, Loss 0.0398, Time 30.4s\n",
            "Step 600, Loss 0.0325, Time 35.7s\n",
            "Step 700, Loss 0.0324, Time 39.6s\n",
            "Step 800, Loss 0.0342, Time 43.6s\n",
            "Step 900, Loss 0.0328, Time 48.9s\n",
            "Step 1000, Loss 0.0304, Time 52.9s\n",
            "Step 1100, Loss 0.0298, Time 56.9s\n",
            "Step 1200, Loss 0.0301, Time 62.1s\n",
            "Step 1300, Loss 0.0297, Time 66.1s\n",
            "Step 1400, Loss 0.0259, Time 70.1s\n",
            "Step 1500, Loss 0.0273, Time 75.3s\n",
            "Step 1600, Loss 0.0266, Time 79.3s\n",
            "Step 1700, Loss 0.0272, Time 83.2s\n",
            "Step 1800, Loss 0.0261, Time 88.5s\n",
            "Step 1900, Loss 0.0236, Time 92.5s\n",
            "Step 2000, Loss 0.0259, Time 96.5s\n"
          ]
        }
      ],
      "source": [
        "# Instantiate the network and print information\n",
        "hidden_size = 128\n",
        "net = RNNNet(input_size=input_size, hidden_size=hidden_size,\n",
        "             output_size=output_size, dt=env.dt)\n",
        "print(net)\n",
        "\n",
        "def train_model(net, dataset):\n",
        "    \"\"\"Simple helper function to train the model.\n",
        "\n",
        "    Args:\n",
        "        net: a pytorch nn.Module module\n",
        "        dataset: a dataset object that when called produce a (input, target output) pair\n",
        "\n",
        "    Returns:\n",
        "        net: network object after training\n",
        "    \"\"\"\n",
        "    # Use Adam optimizer\n",
        "    optimizer = optim.Adam(net.parameters(), lr=0.01)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    running_loss = 0\n",
        "    running_acc = 0\n",
        "    start_time = time.time()\n",
        "    # Loop over training batches\n",
        "    print('Training network...')\n",
        "    for i in range(2000):\n",
        "        # Generate input and target, convert to pytorch tensor\n",
        "        inputs, labels = dataset()\n",
        "        inputs = torch.from_numpy(inputs).type(torch.float)\n",
        "        labels = torch.from_numpy(labels.flatten()).type(torch.long)\n",
        "\n",
        "        # boiler plate pytorch training:\n",
        "        optimizer.zero_grad()   # zero the gradient buffers\n",
        "        output, _ = net(inputs)\n",
        "        # Reshape to (SeqLen x Batch, OutputSize)\n",
        "        output = output.view(-1, output_size)\n",
        "        loss = criterion(output, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()    # Does the update\n",
        "\n",
        "        # Compute the running loss every 100 steps\n",
        "        running_loss += loss.item()\n",
        "        if i % 100 == 99:\n",
        "            running_loss /= 100\n",
        "            print('Step {}, Loss {:0.4f}, Time {:0.1f}s'.format(\n",
        "                i+1, running_loss, time.time() - start_time))\n",
        "            running_loss = 0\n",
        "    return net\n",
        "\n",
        "net = train_model(net, dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "affected-divide"
      },
      "source": [
        "## Testing the network\n",
        "\n",
        "Here we run the network after training, record activity, and compute performance. We will explicitly loop through individual trials, so we can log the information and compute the performance of each trial."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "yellow-jason",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4b41547-cbc2-4964-90ff-7519744ce1a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial  0 {'ground_truth': 0, 'coh': 0.0, 'correct': True}\n",
            "Trial  1 {'ground_truth': 0, 'coh': 6.4, 'correct': True}\n",
            "Trial  2 {'ground_truth': 0, 'coh': 51.2, 'correct': True}\n",
            "Trial  3 {'ground_truth': 1, 'coh': 25.6, 'correct': True}\n",
            "Trial  4 {'ground_truth': 0, 'coh': 25.6, 'correct': True}\n",
            "Average performance 0.925\n"
          ]
        }
      ],
      "source": [
        "# Reset environment\n",
        "env = dataset.env\n",
        "env.reset(no_step=True)\n",
        "\n",
        "# Initialize variables for logging\n",
        "perf = 0\n",
        "activity_dict = {}  # recording activity\n",
        "trial_infos = {}  # recording trial information\n",
        "\n",
        "num_trial = 200\n",
        "for i in range(num_trial):\n",
        "    # Neurogym boiler plate\n",
        "    # Sample a new trial\n",
        "    trial_info = env.new_trial()\n",
        "    # Observation and groud-truth of this trial\n",
        "    ob, gt = env.ob, env.gt\n",
        "    # Convert to numpy, add batch dimension to input\n",
        "    inputs = torch.from_numpy(ob[:, np.newaxis, :]).type(torch.float)\n",
        "\n",
        "    # Run the network for one trial\n",
        "    # inputs (SeqLen, Batch, InputSize)\n",
        "    # action_pred (SeqLen, Batch, OutputSize)\n",
        "    action_pred, rnn_activity = net(inputs)\n",
        "\n",
        "    # Compute performance\n",
        "    # First convert back to numpy\n",
        "    action_pred = action_pred.detach().numpy()[:, 0, :]\n",
        "    # Read out final choice at last time step\n",
        "    choice = np.argmax(action_pred[-1, :])\n",
        "    # Compare to ground truth\n",
        "    correct = choice == gt[-1]\n",
        "\n",
        "    # Record activity, trial information, choice, correctness\n",
        "    rnn_activity = rnn_activity[:, 0, :].detach().numpy()\n",
        "    activity_dict[i] = rnn_activity\n",
        "    trial_infos[i] = trial_info  # trial_info is a dictionary\n",
        "    trial_infos[i].update({'correct': correct})\n",
        "\n",
        "# Print information for sample trials\n",
        "for i in range(5):\n",
        "    print('Trial ', i, trial_infos[i])\n",
        "\n",
        "print('Average performance', np.mean([val['correct'] for val in trial_infos.values()]))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Backpropagation Through Time (BPTT)\n",
        "\n",
        "We will now delve into the world of Recurrent Neural Networks (RNNs), gaining an understanding of their functionality and constructing one from the ground up using only NumPy in Python. Having previously explored backpropagation in feedforward neural networks, we now turn our attention to the complexity introduced by temporal dependencies. Frameworks like PyTorch handle BPTT automatically via autograd. Below, we'll modify our training loop to illustrate how BPTT works under the hood.\n",
        "\n",
        "### Implementing BPTT Step-by-Step\n",
        "We'll implement BPTT manually to illustrate how it works. This involves:\n",
        "\n",
        "Forward Pass: Compute the network's output and store necessary variables.\n",
        "Backward Pass: Compute gradients of the loss with respect to weights by backpropagating errors through time.\n",
        "Weight Updates: Update the weights using the computed gradients.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "azWvNxGVvYW5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Forward Pass (Modifying the LeakyRNN Class)\n",
        "\n",
        "We need to store the inputs and hidden states at each time step during the forward pass to use them in the backward pass."
      ],
      "metadata": {
        "id": "wgUakUuL0jsc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LeakyRNN(nn.Module):\n",
        "    \"\"\" LeakyRNN with BPTT support.\"\"\"\n",
        "\n",
        "    def __init__(self, input_size, hidden_size, dt=None, **kwargs):\n",
        "        super().__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.tau = 100\n",
        "        if dt is None:\n",
        "            alpha = 1\n",
        "        else:\n",
        "            alpha = dt / self.tau\n",
        "        self.alpha = alpha\n",
        "\n",
        "        # Define weights explicitly\n",
        "        self.Wxh = nn.Parameter(torch.randn(hidden_size, input_size) * 0.01)\n",
        "        self.Whh = nn.Parameter(torch.randn(hidden_size, hidden_size) * 0.01)\n",
        "        self.bh = nn.Parameter(torch.zeros(hidden_size))\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        return torch.zeros(batch_size, self.hidden_size)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        \"\"\"Forward pass through time, storing variables for BPTT.\"\"\"\n",
        "        seq_len, batch_size, _ = inputs.size()\n",
        "        hidden = self.init_hidden(batch_size).to(inputs.device)\n",
        "\n",
        "        self.inputs = []   # Store inputs for BPTT\n",
        "        self.hiddens = [hidden]  # Store hidden states for BPTT\n",
        "\n",
        "        outputs = []\n",
        "        for t in range(seq_len):\n",
        "            input_t = inputs[t]\n",
        "            self.inputs.append(input_t)\n",
        "            # Compute pre-activation\n",
        "            pre_activation = self.Wxh @ input_t.T + self.Whh @ hidden.T + self.bh[:, None]\n",
        "            pre_activation = pre_activation.T  # Shape: (batch_size, hidden_size)\n",
        "            # Apply activation function\n",
        "            hidden = (1 - self.alpha) * hidden + self.alpha * torch.tanh(pre_activation)\n",
        "            self.hiddens.append(hidden)\n",
        "            outputs.append(hidden)\n",
        "        outputs = torch.stack(outputs)\n",
        "        return outputs\n",
        "\n",
        "class RNNNet(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, **kwargs):\n",
        "        super().__init__()\n",
        "\n",
        "        # Use our modified LeakyRNN\n",
        "        self.rnn = LeakyRNN(input_size, hidden_size, **kwargs)\n",
        "\n",
        "        # Output layer\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        rnn_output = self.rnn(x)\n",
        "        out = self.fc(rnn_output)\n",
        "        return out"
      ],
      "metadata": {
        "id": "81jl1CMq6QPk"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Backward Pass (Manual Gradient Computation)\n",
        "\n"
      ],
      "metadata": {
        "id": "igFhTSyt0SUw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def bptt(net, outputs, targets):\n",
        "    \"\"\"Manual Backpropagation Through Time.\"\"\"\n",
        "    # Initialize gradients\n",
        "    dWxh = torch.zeros_like(net.rnn.Wxh)\n",
        "    dWhh = torch.zeros_like(net.rnn.Whh)\n",
        "    dbh = torch.zeros_like(net.rnn.bh)\n",
        "    dWhy = torch.zeros_like(net.fc.weight)\n",
        "    dby = torch.zeros_like(net.fc.bias)\n",
        "\n",
        "    # Initialize gradient w.r.t hidden state\n",
        "    dh_next = torch.zeros(outputs.size(1), net.rnn.hidden_size)\n",
        "\n",
        "    seq_len, batch_size, num_classes = outputs.size()\n",
        "\n",
        "    # Compute gradient of loss w.r.t. output logits\n",
        "    outputs_flat = outputs.view(-1, num_classes)\n",
        "    outputs_softmax = torch.softmax(outputs_flat, dim=1)\n",
        "    outputs_softmax = outputs_softmax.view(seq_len, batch_size, num_classes)\n",
        "\n",
        "    # Create one-hot encoding of targets\n",
        "    targets_one_hot = torch.nn.functional.one_hot(targets, num_classes=num_classes).float()\n",
        "\n",
        "    # Compute dy = dL/dy (gradient of loss w.r.t. logits)\n",
        "    dy = outputs_softmax - targets_one_hot  # Shape: (seq_len, batch_size, num_classes)\n",
        "\n",
        "    # Loop backward through time\n",
        "    for t in reversed(range(seq_len)):\n",
        "        # Gradients for output layer\n",
        "        ht = net.rnn.hiddens[t+1]  # Hidden state at time t\n",
        "        dWhy += dy[t].T @ ht\n",
        "        dby += dy[t].sum(0)\n",
        "\n",
        "        # Backprop into hidden layer\n",
        "        dh = dy[t] @ net.fc.weight + dh_next  # Shape: (batch_size, hidden_size)\n",
        "\n",
        "        # Derivative through activation function\n",
        "        dtanh = net.rnn.alpha * (1 - ht ** 2) * dh  # Shape: (batch_size, hidden_size)\n",
        "\n",
        "        # Gradients w.r.t parameters\n",
        "        xt = net.rnn.inputs[t]  # Input at time t\n",
        "        ht_prev = net.rnn.hiddens[t]  # Hidden state at time t-1\n",
        "        dWxh += dtanh.T @ xt\n",
        "        dWhh += dtanh.T @ ht_prev\n",
        "        dbh += dtanh.sum(0)\n",
        "\n",
        "        # Prepare dh_next for next iteration\n",
        "        dh_next = dh * (1 - net.rnn.alpha) + dtanh @ net.rnn.Whh.T\n",
        "\n",
        "    # Clip gradients to prevent exploding gradients\n",
        "    clip_value = 1.0\n",
        "    for grad in [dWxh, dWhh, dbh, dWhy, dby]:\n",
        "        grad.clamp_(-clip_value, clip_value)\n",
        "\n",
        "    # Update weights manually\n",
        "    learning_rate = 0.0001\n",
        "    net.rnn.Wxh.data -= learning_rate * dWxh\n",
        "    net.rnn.Whh.data -= learning_rate * dWhh\n",
        "    net.rnn.bh.data -= learning_rate * dbh\n",
        "    net.fc.weight.data -= learning_rate * dWhy\n",
        "    net.fc.bias.data -= learning_rate * dby\n"
      ],
      "metadata": {
        "id": "xsOZQCkh54Nu"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Weight Updates"
      ],
      "metadata": {
        "id": "2p9E7-wE0Sk9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model_bptt(net, dataset):\n",
        "    \"\"\"Train the model using manual BPTT.\"\"\"\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    running_loss = 0\n",
        "    print('Training network with BPTT...')\n",
        "    for i in range(2000):\n",
        "        # Generate input and target, convert to PyTorch tensors\n",
        "        inputs, labels = dataset()\n",
        "        inputs = torch.from_numpy(inputs).type(torch.float)  # Shape: (seq_len, batch_size, input_size)\n",
        "        labels = torch.from_numpy(labels).type(torch.long)   # Shape: (seq_len, batch_size)\n",
        "\n",
        "        # Zero gradients\n",
        "        net.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = net(inputs)  # outputs shape: (seq_len, batch_size, num_classes)\n",
        "\n",
        "        # Compute loss\n",
        "        outputs_flat = outputs.view(-1, outputs.size(-1))    # Shape: (seq_len * batch_size, num_classes)\n",
        "        labels_flat = labels.view(-1)                        # Shape: (seq_len * batch_size)\n",
        "        loss = criterion(outputs_flat, labels_flat)\n",
        "\n",
        "        # Backward pass using manual BPTT\n",
        "        bptt(net, outputs, labels)\n",
        "\n",
        "        # Logging\n",
        "        running_loss += loss.item()\n",
        "        if i % 100 == 99:\n",
        "            running_loss /= 100\n",
        "            print('Step {}, Loss {:0.4f}'.format(i+1, running_loss))\n",
        "            running_loss = 0\n",
        "    return net\n"
      ],
      "metadata": {
        "id": "W3lzuKO17mup"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training the Network with BPTT"
      ],
      "metadata": {
        "id": "p75aoq6L8OKT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate the network\n",
        "hidden_size = 128\n",
        "net = RNNNet(input_size=input_size, hidden_size=hidden_size,\n",
        "             output_size=output_size, dt=env.dt)\n",
        "\n",
        "# Train the network\n",
        "net = train_model_bptt(net, dataset)\n"
      ],
      "metadata": {
        "id": "O8JVRPtf8Phf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "056fe84a-bf46-499a-c679-d4ca48b593c3"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training network with BPTT...\n",
            "Step 100, Loss 0.9611\n",
            "Step 200, Loss 0.4608\n",
            "Step 300, Loss 0.4134\n",
            "Step 400, Loss 0.3899\n",
            "Step 500, Loss 0.3681\n",
            "Step 600, Loss 0.3494\n",
            "Step 700, Loss 0.3289\n",
            "Step 800, Loss 0.3077\n",
            "Step 900, Loss 0.2893\n",
            "Step 1000, Loss 0.2703\n",
            "Step 1100, Loss 0.2516\n",
            "Step 1200, Loss 0.2358\n",
            "Step 1300, Loss 0.2201\n",
            "Step 1400, Loss 0.2049\n",
            "Step 1500, Loss 0.1925\n",
            "Step 1600, Loss 0.1803\n",
            "Step 1700, Loss 0.1685\n",
            "Step 1800, Loss 0.1589\n",
            "Step 1900, Loss 0.1493\n",
            "Step 2000, Loss 0.1396\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing the network"
      ],
      "metadata": {
        "id": "KaLVcE2S8UwZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_model(net, env, num_trial=200):\n",
        "    # Initialize variables for logging\n",
        "    activity_dict = {}  # recording activity\n",
        "    trial_infos = {}  # recording trial information\n",
        "    for i in range(num_trial):\n",
        "        # Sample a new trial\n",
        "        trial_info = env.new_trial()\n",
        "        # Observation and ground-truth of this trial\n",
        "        ob, gt = env.ob, env.gt\n",
        "        # Convert to tensor, add batch dimension to input\n",
        "        inputs = torch.from_numpy(ob[:, np.newaxis, :]).type(torch.float)\n",
        "        # Run the network for one trial\n",
        "        outputs = net(inputs)\n",
        "        outputs = outputs.detach().numpy()[:, 0, :]\n",
        "        # Compute performance\n",
        "        choice = np.argmax(outputs[-1, :])\n",
        "        correct = choice == gt[-1]\n",
        "        # Record activity, trial information, choice, correctness\n",
        "        activity_dict[i] = outputs\n",
        "        trial_infos[i] = trial_info  # trial_info is a dictionary\n",
        "        trial_infos[i].update({'correct': correct})\n",
        "    return trial_infos, activity_dict\n"
      ],
      "metadata": {
        "id": "KMfNkC1e32C9"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trial_infos, activity_dict = test_model(net, env, num_trial=200)\n",
        "print('Average performance', np.mean([val['correct'] for val in trial_infos.values()]))\n"
      ],
      "metadata": {
        "id": "5iah1FZ58W7S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56ab06f5-8943-42da-d3f1-2a8425f94de0"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average performance 0.715\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "*Acknowledgments*\n",
        "\n",
        "*Special thanks to Guangyu Robert Yang for their [original work](https://github.com/gyyang/nn-brain/blob/master/RNN_tutorial.ipynb), which served as a foundation for this tutorial.*"
      ],
      "metadata": {
        "id": "XY62ZtMT-xhx"
      }
    }
  ]
}