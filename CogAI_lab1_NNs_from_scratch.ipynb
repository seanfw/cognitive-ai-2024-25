{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Feedforward neural network\n",
        "\n",
        "We're exploring a feedforward neural network with multiple interconnected artificial neurons. In this initial section, you'll implement the feedforward pass (representing the propagation of neural activity through the network) for a basic task. In future notebooks, you'll learn how to train this network to improve its\n",
        " accuracy.\n"
      ],
      "metadata": {
        "id": "zu2TXB6FJ8MG"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wT7f7avgUELA"
      },
      "source": [
        "\n",
        "## 1.1 Intro\n",
        "\n",
        "In this task, we aim to simulate the activity of a simplified feedforward neuronal network that processes input signals (stimuli/features) and maps them to the desired outputs (behavioral responses/targets). These inputs, $\\mathbf{x}$, can be thought of as a stimulus pattern received by the sensory neurons, while the outputs, $\\mathbf{y}$, represent the downstream target, akin to correct motor or cognitive outputs.\n",
        "\n",
        "Here we are presented with a training dataset $\\mathbf{D}$, which gives $N$ examples of correct outputs for several different input stimuli ($(\\mathbf{x}_i, y_i)_{i=1}^N$).\n",
        "\n",
        "We train a neural network with a single hidden layer to perform logical computations akin to simple neural circuits that process binary signals (like spikes). These logic gates (AND, OR, XOR, XNOR) can be thought of as simplified forms of decision-making processes in the brain. Neurons act as nonlinear integrators of incoming signals, which allows them to compute functions similar to logical gates.\n",
        "\n",
        "In our model, the input pairs \\(A\\) and \\(B\\) represent binary signals (spikes or no spikes), and the network processes these inputs through interconnected layers to produce an output signal, $\\mathbf{y}$, analogous to the firing of a downstream neuron.\n",
        "\n",
        "The following table shows the neural network's output for different combinations of inputs \\(A\\) and \\(B\\), with each output corresponding to a different logical gate. Each row corresponds to one example with binary inputs \\(A\\) and \\(B\\) and the correct binary outputs (AND, OR, XOR, XNOR):\n",
        "\n",
        "\n",
        "\\begin{array}{|c|c|c|c|c|c|}\n",
        "\\hline\n",
        "A & B & AND & OR & XOR & XNOR \\\\\n",
        "\\hline\n",
        "0 & 0 & 0 & 0 & 0 & 1 \\\\\n",
        "0 & 1 & 0 & 1 & 1 & 0 \\\\\n",
        "1 & 0 & 0 & 1 & 1 & 0 \\\\\n",
        "1 & 1 & 1 & 1 & 0 & 1 \\\\\n",
        "\\hline\n",
        "\\end{array}\n",
        "\n",
        "\n",
        "In this way, the neural network, with its simple architecture, replicates fundamental decision-making or signal-processing operations akin to neural circuits.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9IhFVvGpUELR"
      },
      "source": [
        "##1.2 Importing libraries\n",
        "\n",
        "First, let's import the required libraries. Instead of relying on specialized machine learning libraries like pytorch, we'll handle array operations ourselves with numpy. For visualization, we'll utilize matplotlib."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "tQvz4AIMUELT"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gbU1iFKoUELY"
      },
      "source": [
        "## 1.3 Preparing the Data Sets\n",
        "It's time to organize our data. Given that we have just four unique input combinations (as detailed in the previous table), our task is fairly straightforward. We'll capture the dataset in the form of a list containing possible $(\\mathbf{u}, v)$ pairs, using the terminology mentioned earlier. The data for the AND operation has already been provided. Your initial challenge is to define the datasets for the OR, XOR, and XNOR operations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UukdrH7mUELZ"
      },
      "outputs": [],
      "source": [
        "#the dataset defined by the logical operator AND\n",
        "dataset_and = [\n",
        "                ([0, 0], 0),\n",
        "                ([0, 1], 0),\n",
        "                ([1, 0], 0),\n",
        "                ([1, 1], 1)\n",
        "                ]\n",
        "\n",
        "#the dataset defined by the logical operator OR\n",
        "dataset_or = \"TODO\"\n",
        "\n",
        "#the dataset defined by the logical operator XOR\n",
        "dataset_xor = \"TODO\"\n",
        "\n",
        "#the dataset defined by the logical operator XNOR\n",
        "dataset_xnor = \"TODO\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fz2ZfyAnUELb"
      },
      "source": [
        "Batch processing can speed up the learning process. This is because, instead of looping through data points one by one, we can process entire arrays of data simultaneously.\n",
        "\n",
        "\n",
        "Here we introduce a 'generate_batch' function. This method will:\n",
        "\n",
        "- Accept a user-defined dataset and batch size as inputs.\n",
        "- Produce a batch of the specified size with appropriate examples from the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qKOMdrDDUELd"
      },
      "outputs": [],
      "source": [
        "# Batch Creation: Using a dataset D and a specified batch_size N, this function\n",
        "# selects N random samples from D. It returns a tuple containing ordered arrays:\n",
        "# (inputs, targets). For each entry, targets[i] matches the target for inputs[i].\n",
        "\n",
        "def generate_batch(dataset, batch_size):\n",
        "    #differentiate inputs (features) from targets and transform each into\n",
        "    #numpy array with each row as an example\n",
        "    inputs = np.vstack([ex[0] for ex in dataset])\n",
        "    targets = np.vstack([ex[1] for ex in dataset])\n",
        "\n",
        "    #randomly choose batch_size many examples; note there will be\n",
        "    #duplicate entries when batch_size > len(dataset)\n",
        "    rand_inds = np.random.randint(0, len(dataset), batch_size)\n",
        "    inputs_batch = inputs[rand_inds]\n",
        "    targets_batch = targets[rand_inds]\n",
        "\n",
        "    return inputs_batch, targets_batch\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VXF8KVp0UELe"
      },
      "source": [
        "We can now generate batches for any of the datasets defined above. As an example, to obtain 100 examples out of the XOR dataset, we just run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D3N13KRJUELf",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "inputs_xor, targets_xor = generate_batch(dataset_xor, batch_size=100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0_xzTlfyUELh"
      },
      "source": [
        "## 1.4 Network Initialization\n",
        "\n",
        "Let's build our single hidden layer neural network from scratch in python, but in the style of [Pytorch](https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html).\n",
        "\n",
        "We'll define a class, *nn_one_layer*, which will be initialized with the following hyperparameters:\n",
        "\n",
        "\n",
        "*   input_size:  the dimensionality of the input stimuli.\n",
        "*   hidden_size: the hidden layer's dimensionality, the number of hidden units or \"neurons.\"\n",
        "*   target_size: the output size.\n",
        "\n",
        "The hyperparameters also guide the dimensions of the network's weight matrices. Here we have two weight matrices, the input weights $W_{a←x}$ and the output weights $W_{y←a}$. When provided with the input stimuli as a vector, $\\mathbf{x}$, the network produces the output, $\\hat{y}$, via:\n",
        "\n",
        "\\begin{align}\n",
        "\\mathbf{a} &= f(\\mathbf{x}W_{a←x}) \\\\\n",
        "\\hat{y} &= \\mathbf{a}W_{y←a}\n",
        "\\end{align}\n",
        "\n",
        "Where $\\mathbf{a}$ is the 'hidden layer activity' of the network and $f$ is the _activation function_, which determines how the (synaptic) inputs to a neuron are converted into an output (e.g. firing rate). This is usually a non-linear function, and here we will take $f$ to be the sigmoid function, $f(z) =\\frac{1}{1+e^{-z}}$. As the output dimension is 1 for this task, we can just use a scalar for the network output $y$.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### 2.4.1 Matrix Representation of Equations\n",
        "\n",
        "To allow for batch processing, let's switch to matrix notation.\n",
        "\n",
        "Let $b_s$ denote the batch size, and $i_s$, $h_s$ and $o_s$ the input, hidden and output size respectively. Now we can write the dimension of the input matrix $\\mathbf{X}$ as $(b_s, i_s)$. The equations for the 'batch hidden activity' $\\mathbf{H}$ and 'batch output' $\\mathbf{y}$ is just the same as before\n",
        "\n",
        "\\begin{align}\n",
        "\\mathbf{Z} &= \\mathbf{X}W_{a←x} \\\\\n",
        "\\mathbf{A} &= f(\\mathbf{Z}) \\\\\n",
        "\\hat{\\mathbf{y}} &= \\mathbf{A}W_{y←a}\n",
        "\\end{align}\n",
        "\n",
        "Understanding Check: Can you determine the dimensions of $\\mathbf{A}$ and $\\hat{\\mathbf{y}}$ based on $i_s$, $h_s$, $o_s$, and $b_s$? How about the dimensions of $W_{a←x}$ and $W_{y←a}$?\n",
        "\n",
        "Fill in the 'TODO's below to build one layer of the neural network."
      ],
      "metadata": {
        "id": "arZcv1V-KCGI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "U9m-1naCUELi"
      },
      "outputs": [],
      "source": [
        "#code for *nn_one_layer* which initiliases the model and also defines the forward\n",
        "#computation. we will initialise each weight randomly according to a normal\n",
        "#distribution of standard deviation sigma = 0.1.\n",
        "\n",
        "def sigmoid(a):\n",
        "  # define the sigmoid function (act)\n",
        "    f_sigmoid = \"TODO\"\n",
        "    return f_sigmoid\n",
        "\n",
        "\n",
        "class nn_one_layer():\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        #define the input/output weights Wax, Wya, with standard deviation sigma = 0.1.\n",
        "        self.Wax = 0.1 * np.random.randn(input_size, hidden_size)\n",
        "        self.Wya = \"TODO\"\n",
        "\n",
        "        self.f = sigmoid\n",
        "\n",
        "    def forward(self, x):\n",
        "      # hint: use np.matmul to perform matrix multiplication\n",
        "      # hint 2: remember to write 'self' where used in definitions above\n",
        "        z = \"TODO\"\n",
        "        a = \"TODO\"\n",
        "        y = \"TODO\"\n",
        "        return y, a, z"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g7Bxz2xGUELi"
      },
      "source": [
        "You should now be able to initialise your model and check it can perform a batch computation on the datasets defined above.\n",
        "\n",
        "### 2.4.2 histogram of output values\n",
        "Fill in the 'TODO's below to view a scatter plot and a histogram of output values for the XOR dataset as predicted by an untrained neural network of 5 hidden neurons."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s3BGvAxXUELj"
      },
      "outputs": [],
      "source": [
        "input_size = \"TODO\"\n",
        "hidden_size = \"TODO\"\n",
        "output_size = \"TODO\"\n",
        "\n",
        "nn = nn_one_layer(input_size, hidden_size, output_size) #initialise model\n",
        "preds_xor, _, _ = nn.forward(inputs_xor) #prediction made by model on batch xor input\n",
        "\n",
        "_, inds = np.unique(inputs_xor, return_index=True, axis=0)\n",
        "\n",
        "\n",
        "# plot target vs predictions along with a histogram of each\n",
        "fig = plt.figure()\n",
        "ax1 = plt.subplot2grid((2,2), (0,0), rowspan=1, colspan=2)\n",
        "plt.scatter(targets_xor[inds], preds_xor[inds], marker='x', c='black')\n",
        "for i in inds:\n",
        "    coord = '({}, {})'.format(inputs_xor[i][0], inputs_xor[i][1])\n",
        "    xoffset = 0.05 if targets_xor[i] == 0 else -0.1\n",
        "    yoffset = 0.003 if preds_xor[i] > np.mean(preds_xor[inds]) else -0.005\n",
        "    plt.text(targets_xor[i] + xoffset, preds_xor[i] + yoffset, coord)\n",
        "plt.xlabel('target values')\n",
        "plt.ylabel('predicted values')\n",
        "plt.ylim([np.min(preds_xor) - 0.01, np.max(preds_xor) + 0.01])\n",
        "ax2 = plt.subplot2grid((2,2), (1,0), rowspan=1, colspan=1)\n",
        "plt.hist(targets_xor, color='blue')\n",
        "ax2.set_title('target values')\n",
        "plt.ylabel('# in batch')\n",
        "ax3 = plt.subplot2grid((2,2), (1,1), rowspan=1, colspan=1, sharey=ax2)\n",
        "plt.hist(preds_xor, color='red')\n",
        "ax3.set_title('predicted values')\n",
        "\n",
        "fig.tight_layout()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jUY2flafUELl"
      },
      "source": [
        "If you managed to generate the plot, hopefully you see that the network is pretty useless at the moment! It doesn't get near the required target values according to the true XOR operator, let alone understanding that it should only produce logical output vaues of 0 or 1.\n",
        "\n",
        "The expectation is that, with a good learning procedure to train the weights $W_1$ and $W_2$, we should be able to produce a network which better captures XOR, or indeed any other logical gate we fancy.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.5 Exploration & Experimentation time\n",
        "\n",
        "It's exploration time! Modify the parameters and see how it impacts the predictions. Also try generating AND, OR and XNOR datasets and plotting the histograms of output values."
      ],
      "metadata": {
        "id": "GJKx7nbJKF7s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Introduction to RNN Dynamics\n",
        "\n",
        "The objective of this section is to understand the forward pass of a recurrent neural network, which roughly corresponds to the dynamics of neural activity in a recurrently connected part of the brain, such as the prefrontal cortex.\n"
      ],
      "metadata": {
        "id": "zkKRxyibKMGk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## 3.1 Introduction to RNN Dynamics:\n",
        "\n",
        "RNNs are a type of neural network with loops, allowing information persistence. Unlike traditional feedforward neural networks, RNNs share parameters across different steps in this sequence. Here we will implement a type of RNN called a 'leaky RNN', which produces reasonably brain-like dynamics:\n",
        "\n",
        "$$\n",
        "\\tau\\frac{d a_{i}(t)}{d t} = -a_{i}(t) + f(\\sum_{j=1}^{N} w_{i j} a_{j}(t) + x_{i}(t)).\n",
        "$$\n",
        "\n",
        "Here's a breakdown of each term:\n",
        "\n",
        "- $\\frac{d a_{i}(t)}{d t}$: Represents the how much the firing rate $a$ of a neuron $i$ changes over a short step in time $dt$.\n",
        "\n",
        "- $a_{i}(t)$: the firing rate of neuron $i$ at time $t$. The term $-a_{i}(t)$ means that, without any new synaptic inputs, the firing rate would decay exponentially towards zero.\n",
        "\n",
        "- $w_{i j}$: This represents the weight from neuron $j$ to neuron $i$ within the recurrent layer:. It dictates how much of neuron $j$'s state should influence neuron $i$. (in the slides this is represented by the matrix $W_{a→a}$)\n",
        "\n",
        "- $f()$: the activation (input-output) function\n",
        "\n",
        "- $\\sum_{j=1}^{N}$: This is a summation over all neurons in the network, indicating that the next state of neuron $i$ is influenced by all neurons, including itself.\n",
        "\n",
        "- $x_{i}(t)$: Represents any stimulus input (from the input neurons) to neuron $i$ at time $t$.\n",
        "\n",
        "Here we assume that the input weight matrix (connecting input stimuli to the recurrently-connected neurons) is the identity matrix, and can thus be ignored.\n",
        "\n",
        "The discrete time version (obtained using the Euler discretisation), which we use to code the model, is similar:\n",
        "\n",
        "$$\n",
        "a_{i}(t) = a_{i}(t-1) + \\frac{Δt}{τ} \\left(-a_{i}(t-1) + f(\\sum_{j=1}^{N} w_{i j} a_{j}(t-1) + x_{i}(t-1)\\right).\n",
        "$$\n",
        "\n",
        "\n",
        "The equation as a whole gives a sense of \"memory\" to the network, where the future state of a neuron is determined by its current state, the states of all other neurons, and any external input."
      ],
      "metadata": {
        "id": "XTxO5uf9G8AB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.2 Understanding Activation Functions\n",
        "\n",
        "Activation functions introduce non-linearity into neural networks, allowing neural networks to tackle more complex problems.\n",
        "\n",
        "Let's visualize some common activation functions:"
      ],
      "metadata": {
        "id": "EBEC2jrlKT-2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define common activation functions\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "# hint, np.tanh and np.maximum could come in handy\n",
        "def tanh(x):\n",
        "    return \"TODO\"\n",
        "\n",
        "def relu(x):\n",
        "    return \"TODO\"\n",
        "\n",
        "# Plotting\n",
        "x = np.linspace(-10, 10, 100)\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.plot(x, sigmoid(x))\n",
        "plt.title(\"Sigmoid Activation\")\n",
        "\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.plot(x, tanh(x))\n",
        "plt.title(\"Tanh Activation\")\n",
        "\n",
        "plt.subplot(1, 3, 3)\n",
        "plt.plot(x, relu(x))\n",
        "plt.title(\"ReLU Activation\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "30kLJdM-J-Uh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.3 Forward Pass (neural dynamics) Demonstration\n",
        "\n",
        "\n",
        "We'll now demonstrate how an RNN processes a sequence of input spikes and produces a dynamic activity pattern."
      ],
      "metadata": {
        "id": "9aC4mNeKKN5x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "N = 3  # number of neurons\n",
        "sequence = [0, 1, 0, 1]\n",
        "dt = 1\n",
        "tau = 1\n",
        "\n",
        "# Random initializations\n",
        "W = np.random.randn(N, N)\n",
        "a = np.random.randn(N)\n",
        "\n",
        "states = [a]\n",
        "\n",
        "# Forward pass through the sequence\n",
        "for x_t in sequence:\n",
        "    da = \"TODO\"  # Using the RNN dynamics equation, and the tanh activation function\n",
        "    a = a + (dt/tau)*da\n",
        "    states.append(a)\n",
        "\n",
        "print(\"Firing rates over time for the provided sequence:\", states)\n"
      ],
      "metadata": {
        "id": "WDG-2DBkLAg8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.4 Sequence Generation with Initialized RNN\n",
        "\n",
        "An RNN with random weights and an initial state can generate a sequence without any external input. Here, we'll set our external input\n",
        "$h_{i}(t)$ to zero and observe the sequence generation based solely on the RNN's initial state and dynamics. Let's observe this behavior:"
      ],
      "metadata": {
        "id": "_b96A47SMXpz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def forward_pass(W, x, initial_state, num_steps=5):\n",
        "\n",
        "    a = initial_state\n",
        "    x = np.zeros(N)  # External input is zero\n",
        "    dt = 1\n",
        "    tau = 1\n",
        "\n",
        "    states = [a]\n",
        "\n",
        "\n",
        "    for _ in range(num_steps):\n",
        "        da = \"TODO\"\n",
        "        a = a + (dt/tau)*da\n",
        "        states.append(a)\n",
        "\n",
        "    return states\n",
        "\n",
        "initial_state = np.random.randn(N)\n",
        "generated_sequence = forward_pass(W, x, initial_state)\n",
        "print(\"Generated sequence:\", generated_sequence)\n"
      ],
      "metadata": {
        "id": "7wrlt-SaMhOF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.5 Visualizing Generated Sequences\n",
        "\n"
      ],
      "metadata": {
        "id": "vpUAAZkxK6Zh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the generated sequence\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# If you're working with multi-dimensional states, you can plot each dimension separately\n",
        "for i in range(N):\n",
        "    plt.plot([state[i] for state in generated_sequence], label=f\"Neuron {i+1}\")\n",
        "\n",
        "plt.title(\"Generated RNN States Over Time\")\n",
        "plt.xlabel(\"Time Steps\")\n",
        "plt.ylabel(\"State Value\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "um7DFIpRLOHq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the plot, observe how the firing rates of the different neurons evolve over time."
      ],
      "metadata": {
        "id": "WL-EVhTZLVKo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.6 Exploration & Experimentation time\n",
        "\n",
        "It's exploration time! Modify the RNN parameters and see how it impacts the sequence generation. Change the weights, initial states, or even the activation function.\n",
        "\n",
        "Now you have coded up the forward pass of a simple feedforward neural network, and a  simple recurrent neural network, which corresponds roughly to the dynamic neural activity in the brain. Unfortunately, your networks aren't great at solving tasks yet. For that, they need to learn (up next time)."
      ],
      "metadata": {
        "id": "XUIYHkWnM5Xq"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3NT6VfjlMOXa"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "celltoolbar": "Attachments",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}